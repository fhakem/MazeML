{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IA318 - Reinforcement Learning\n",
    "\n",
    "# Dynamic programming\n",
    "\n",
    "This notebook presents techniques of dynamic programming (**policy iteration, value iteration**) for the Walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Walk, Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = 'Wistia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman's equation\n",
    "\n",
    "The value of a policy is the unique solution $V$ to Bellman's equation:\n",
    "$$\n",
    "V = P(R + \\gamma V)\n",
    "$$\n",
    "where $P$ is the transition matrix, $R$ is the reward vector and $\\gamma$ the discount factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition matrix in sparse format\n",
    "def get_transition_matrix(policy):\n",
    "    n = Walk.Length\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    for state in range(n):\n",
    "        for prob, action in zip(*policy(state)):\n",
    "            probs, states = Walk.get_transition(state, action)\n",
    "            row += len(states) * [state]\n",
    "            col += list(states)\n",
    "            data += list(prob * np.array(probs))\n",
    "    transition = sparse.csr_matrix((data, (row, col)), shape=(n, n))\n",
    "    return transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rewards():\n",
    "    n = Walk.Length\n",
    "    rewards = [Walk.get_reward(state) for state in range(n)]\n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(policy, gamma=0.9, n_iter=100): \n",
    "    n = Walk.Length\n",
    "    values = np.zeros(n)\n",
    "    transition = get_transition_matrix(policy)\n",
    "    rewards = get_rewards()\n",
    "    for t in range(n_iter):\n",
    "        values = transition.dot(rewards + gamma * values)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first policy (here random)\n",
    "policy = Agent(Walk()).policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "values = evaluate(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.53627941, 2.81809481, 2.72617966, 2.12898424, 2.00492262,\n",
       "       2.32641249, 3.16490916, 4.70673207, 5.07229953, 6.56505773])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_values(values):\n",
    "    plt.imshow(values.reshape(1,-1), cmap=cmap)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAABIElEQVR4nO3aMUqDURSE0fkTISBWoktwBW7QfbkEFyBYC5aChWgRn0VqLecWOad9zVQft3jbWisAdOymBwCcE9EFKBJdgCLRBSgSXYCii39fX67HvzZ83d5NT0iSHHeH6Qn5uLqfnpAk+dlfTk/I4ft1ekKS5Ob9cXpC8vQ2veDkc3pAcnyeXnCyf1jbX28uXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2Aom2tNb0B4Gy4dAGKRBegSHQBikQXoEh0AYpEF6DoF1ADFll1zpR4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_values(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_targets():\n",
    "    n = Walk.Length\n",
    "    rewards = np.zeros(n)\n",
    "    rewards[Walk.Reward_States] = Walk.Reward_Values\n",
    "    image = plt.imshow(rewards.reshape(1,-1), cmap=cmap)    \n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAA90lEQVR4nO3asQ0CMRAAQT+iCCTKoStISKArykGiC1MBhGskZtJLLrmVA29zzgFAY7d6AYB/IroAIdEFCIkuQEh0AUL7b8PnuCz/2nB83FevMMYY43k6r14Bftrhuv5WX9ffuNPjuG2fZl66ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6AKFtzrl6B4C/4aULEBJdgJDoAoREFyAkugAh0QUIvQE9WQ9ZVQJolQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_targets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(values):\n",
    "    n = Walk.Length\n",
    "    best_actions = []\n",
    "    for state in range(n):\n",
    "        actions = Walk.get_actions(state)\n",
    "        rewards = []\n",
    "        for action in actions:\n",
    "            probs, states = Walk.get_transition(state, action)\n",
    "            rewards.append(np.sum(np.array(probs) * values[states]))\n",
    "        best_actions.append(actions[np.argmax(rewards)])\n",
    "    # policy(state) -> probs, actions\n",
    "    policy = lambda state: [[1], [best_actions[state]]]\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(policy, state):\n",
    "    probs, actions = policy(state)\n",
    "    return actions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = improve_policy(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_policy(policy):\n",
    "    n = Walk.Length\n",
    "    rewards = np.zeros(n)\n",
    "    rewards[Walk.Reward_States] = Walk.Reward_Values\n",
    "    plt.imshow(rewards.reshape(1,-1), cmap=cmap)   \n",
    "    marker_sign = {1: '>', -1:'<'}\n",
    "    for action, marker in marker_sign.items():\n",
    "        x = [state for state in range(n) if get_action(policy, state) == action]\n",
    "        if len(x):\n",
    "            y = np.zeros_like(x)\n",
    "            plt.scatter(x, y, marker=marker, s=200, c='grey')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAEg0lEQVR4nO3dwWobVxjF8f+diUIXCfIyYOgihS4iGkq6b2zyAn4KP4Sz8SaCvIILJW8Qt/tgd9VVS3FsL5qSQMAQ2i4UvDG2594uHIFROq4qzf3uZ+X8tjLiaO79DiN7ZhxSSoiIiI2qdAARkU+JSldExJBKV0TEkEpXRMSQSldExNCNq1484nHxSxuWXwxLRwDg6NFG6Qgirt3ZLD+r7zZ9zOkyT0LbazrTFRExpNIVETGk0hURMaTSFRExpNIVETGk0hURMaTSFRExpNIVETHUSele56dDptR6DbMpD8ew6wyL+Jlm5SGHhwwACR8zN4sujuHcpXt2Cs+GPX7arjkezR/Iyuikz/Pf1xj+vMFpc7NYjuMR7D6veTbscXa6OBk87AsPGbzk8JAB4JSbPGWDH1ljRL9ckP+pyxm58jbgaTQNpAh/vKx4fVBxdxB5sNJwe2ned85jdNJn5+0qh38NiKmCAE2sobbNcTyCX3Zq3hxWxAiBi2PZW5AMHvaFhwxecnjIANBQE6nY5ysOGXCPA75lhyXe2waZUo4Zmbt0AQiQmkDjYFHbTJZt/NCyNeemOSYXMcWLr1qhtvvuZ5bBw77wkMFLDg8ZgECiCTeI1Ownn+Wbc0a6Kd1LUiy/qJe1la21tkX8VDJ42BceMnjJ4SEDQAy1q/K1mJHOS3es9KKOTvrsvl3lwEHZ/rpb8/qgbNmWzjBWel94yeAlh4cMUL58LWckW+mOWS+qytZXhjYeht1DBi85PGQA+/ItMSPZS3cs96KqbH1lmJaHYfeQwUsODxkgf/mWnBHzmyNSDDTngVd7FT981yPG+d8zpsDWb+vs/Xmf89QrVrgxwvZWj1d7Fc15KFJ2HjLMIse+uI4ZvOTwkAEuyvc89HjJfb5nndjBNb6lZ8TsTHcsVImqgruDyDerDVUHtV+FxPrXW8XPdKsK1tbPip5lesgwixz74jpm8JLDQwaAKjVURO6xz0N2qZj/6oHSM2JWupOLeKvj66KXPnvP2pfbrHy+U7R8by/Bw7WGBytNsUX1kGFauffFdcngJYeHDPBx2fY7/p1uyRnJXrrWi6jy9ZWhjYfh9pDBSw4PGSB/2U4qMSPZSrf0Iqp8fWUYK70vvGTwksNDBrAv20mWM9J56V5eRA93pKl8fWTwsC88ZPCSw0MGuFy2Pu5Is5iRbko3+VnENm3la/3Eo7ZF7eDvA/4yeNgXHjJ4yeEhAxdPGfNWtpNyzsjcpVvXECr4wmnZTvqofP8eUFeNeY5/W9Ta+IKLnBk87AsPGbzk8JABoMZ32U7KMSMhXfGAyCMeT9XrKUHIdMK4/GKY540/SCkQwn9/zKNHG5lz5DuGpTIs4me6zjlyZ7izOd2sJgIh01e7d5s+5nSZJ60/1cnVd6U30zymKVwLHo5h1xkW8TPNykMODxmAbIVroYtjqH/XIyJiSKUrImJIpSsiYkilKyJiSKUrImJIpSsiYkilKyJi6MqbI0REpFs60xURMaTSFRExpNIVETGk0hURMaTSFRExpNIVETH0D2CG1Ug49REgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "Try a few steps of policy improvements and observe the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(gamma=0.9, n_iter=100, verbose=True):\n",
    "    n = Walk.Length\n",
    "    agent = Agent(Walk())\n",
    "    policy = agent.policy\n",
    "    change = True\n",
    "    t = 0\n",
    "    while change:\n",
    "        t += 1\n",
    "        # policy evaluation\n",
    "        values = evaluate(policy, gamma, n_iter)\n",
    "        # policy improvement\n",
    "        policy_ = improve_policy(values)\n",
    "        change = bool(np.sum([np.abs(get_action(policy_, state) - get_action(policy, state)) for state in range(n)]))\n",
    "        policy = policy_\n",
    "    if verbose:\n",
    "        print(t)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "* Display the optimal policy obtained by policy iteration.\n",
    "* Observe the impact of the number of iterations.\n",
    "* What happens when this number is equal to 1?\n",
    "* Propose and test an algorithm for value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.improve_policy.<locals>.<lambda>(state)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "<function improve_policy.<locals>.<lambda> at 0x000002FDFEE4A9D8>\n",
      "3\n",
      "<function improve_policy.<locals>.<lambda> at 0x000002FDFEE4A9D8>\n",
      "3\n",
      "<function improve_policy.<locals>.<lambda> at 0x000002FDFEE4A9D8>\n",
      "3\n",
      "<function improve_policy.<locals>.<lambda> at 0x000002FDFEE4A9D8>\n",
      "3\n",
      "<function improve_policy.<locals>.<lambda> at 0x000002FDFEE4A9D8>\n"
     ]
    }
   ],
   "source": [
    "for k in [10, 50,70,90,300]:\n",
    "    print(policy_iteration(n_iter = k))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
